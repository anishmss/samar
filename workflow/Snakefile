from os import path
import itertools

#are read files in fasta format?
if "filetype" not in config: #default to fastq if not specified
    config["filetype"] = "fastq"
if "filetype" in config:
    if config["filetype"] == "fasta":
        convert_fastq_fasta_SE = "cat" #TODO: test this
        interleave = srcdir("scripts/fasta-interleave.sh")
        file_type = "fasta"
    elif config["filetype"] == "fastq":
        convert_fastq_fasta_SE = srcdir("scripts/convert_SE_fastq-fasta.sh")
        interleave = srcdir("scripts/fastq-to-fasta-interleave.sh")
        file_type = "fastq"

#paths to scripts
seq_count = srcdir("scripts/seq_count.py")
trans_6frame = srcdir("scripts/translate_6frames.py")
last_deseq2 = srcdir("scripts/last2deseq.R")


#Input functions
def get_read1(wildcards):
    return config["reads"][wildcards.sample_id][0]

def get_read2(wildcards):
    return config["reads"][wildcards.sample_id][1]

def getMean(wildcard):
    est_path = "{}/last_alignments/{}.frag_len_est".format(config["out_dir"],wildcard)
    if(path.exists(est_path)):
        with open(est_path) as f:
            for line in f:
                if line.startswith("# estimated mean distance"):
                    return(line.rstrip().split()[-1])
    else:
        return(-1)

#Param functions for paired-end fragment stat
def getSTD(wildcard):
    est_path = "{}/last_alignments/{}.frag_len_est".format(config["out_dir"],wildcard)
    if(path.exists(est_path)):
        with open(est_path) as f:
            for line in f:
                if line.startswith("# estimated standard deviation of distance"):
                    return(line.rstrip().split()[-1])
    else:
        return(-1)

def getScoring(wildcards):
    if "training" in config:
        if config["training"].lower() != "no":
            return "{}/score_sample/scoring_scheme".format(config["out_dir"])
        else: return "BL62"
    else:
        return "BL62"


#conditional definition of DE rule
if "deanalysis" not in config:
    config["deanalysis"] = "yes"

if config["deanalysis"].lower() == "no":
    all_input = expand("{out_dir}/counts/{sample_id}/{sample_id}.counts",out_dir=[config["out_dir"]],sample_id=config["reads"].keys())

elif config["deanalysis"].lower() == "yes":
    deseqOutput = ["{0}/DEanalysis/DE_count_summary.txt".format(config["out_dir"]),
                   "{0}/DEanalysis/DESeq2_fit.RDS".format(config["out_dir"])
                ]

    dynamicFiles = "{0}/DEanalysis/".format(config["out_dir"])+"Test_result-{}_{}_vs_{}.csv"

    levels = list(dict(config["sample_info"]).values())
    factorsxlevels = {}

    for n,factor in enumerate(config["factors"]):
        factorsxlevels[factor] = []
        for level in levels:
            if(level[n] not in factorsxlevels[factor]):
                   factorsxlevels[factor].append(level[n])

    for factor in config["design"].split(" + "):
        if("*" not in factor):
            for x in range(1,len(factorsxlevels[factor])):
                deseqOutput.append(dynamicFiles.format(factor,factorsxlevels[factor][x],factorsxlevels[factor][0]))

    for term in config["design"].split("+") :
        if("*" in term):
            interactionTerm = term.replace(" ", "").split("*")
            for order in range(1,len(interactionTerm)):
                for comb in list(itertools.combinations(interactionTerm, order + 1)):
                    coeff = [list(map(lambda x: i+x, factorsxlevels[i][1:])) for i in comb]
                    coeffTests = [ "{}/DEanalysis/Test_result-".format(config["out_dir"]) + '.'.join(i) + ".csv" for i in list(itertools.product(*coeff))]
                    deseqOutput.extend(coeffTests)

    all_input = deseqOutput

    rule deseq: #Rule for performing Differential Expression Analysis
        input:
            expand("{out_dir}/counts/{sample_id}/{sample_id}.counts",out_dir=[config["out_dir"]],sample_id=config["reads"].keys())
        output:
            deseqOutput,
        params:
            count_dir="{0}/counts".format(config["out_dir"]),
            outdir = "{0}/DEanalysis".format(config["out_dir"]),
            sample_info = str(dict(config["sample_info"])).replace("'","\""),
            design = str(config["design"]),
            factors = str(config["factors"]).replace("'","\""),
        conda:
            "env/r.yaml"
        shell: #TODO allow gene-isoform mapping input
            "Rscript {last_deseq2} --in_dir '{params.count_dir}' --factors '{params.factors}'"
            " --design '{params.design}' --sample_info '{params.sample_info}' --out_dir '{params.outdir}'"


# Rule declarations begin here:
rule all: #Initial Rule in snakemake
    input: all_input

rule last_db: #Rule for constructing LAST index
    input:
        reference = config["reference_p"]
    output:
       touch("{0}/last_index/index.done".format(config["out_dir"]))
    params:
       index_basename="{0}/last_index/index".format(config["out_dir"])
    conda:
        "env/lastal.yaml"
    shell:
       "lastdb -p {params.index_basename} {input.reference}"
       #"lastdb {params.index_basename} {input.reference}"

rule last_score_training: #Rule for training score parameters
    input:
        reference_flag = "{0}/last_index/index.done".format(config["out_dir"]),
        query = config["reads"][list(config["reads"].keys())[0]][0],
    params:
        last_index_basename="{0}/last_index/index".format(config["out_dir"]),
        last_sample1="{0}/score_sample/sample_1.sample.fa".format(config["out_dir"]),
        last_trans_sample1="{0}/score_sample/translated.fasta".format(config["out_dir"]),
        file_type=file_type,
    output:
        "{0}/score_sample/scoring_scheme".format(config["out_dir"])
    conda:
        "env/lastal.yaml"
    shell:
        """
        head -n 400000 {input.query} > {params.last_sample1}
        python {trans_6frame} {params.last_sample1} {params.file_type} {params.last_trans_sample1}
        last-train --revsym --matsym --gapsym --sample-number=600000 -S0 {params.last_index_basename} {params.last_trans_sample1}> {output}
        """

rule last_frag_stat_est: #Rule for estimating length distribution of paired-end reads
    input:
        reads1=lambda wildcards: config["reads"][wildcards.sample_id][0],
        reads2=lambda wildcards: config["reads"][wildcards.sample_id][1],
        reference_flag = "{0}/last_index/index.done".format(config["out_dir"])
    params:
        last_index_basename= "{0}/last_index/index".format(config["out_dir"]),
        last_al = "{0}/last_alignments".format(config["out_dir"]),
        scoring = getScoring
    output :
        frag_len_est="{0}/last_alignments/{{sample_id}}.frag_len_est".format(config["out_dir"])
    conda:
        "env/lastal.yaml"
    shell:
        """
        mkfifo {params.last_al}/{wildcards.sample_id}_1
        mkfifo {params.last_al}/{wildcards.sample_id}_2
        gzip -cdf {input.reads1} | head -n 400000 > {params.last_al}/{wildcards.sample_id}_1 &
        gzip -cdf {input.reads2} | head -n 400000 > {params.last_al}/{wildcards.sample_id}_2 &
        {interleave} {params.last_al}/{wildcards.sample_id}_1 {params.last_al}/{wildcards.sample_id}_2 | lastal -i1 -p {params.scoring} -F15 {params.last_index_basename} | last-pair-probs -e > {output.frag_len_est}
        rm {params.last_al}/{wildcards.sample_id}_1 {params.last_al}/{wildcards.sample_id}_2
        """

ruleorder: align_last > align_last_SE

rule align_last: #Rule for aligning paired-end RNA-seq reads to protein database
    input:
        reads1 = lambda wildcards: config["reads"][wildcards.sample_id][0], #Looking at you, snakemake.
        reads2 = lambda wildcards: config["reads"][wildcards.sample_id][1],
        frag_len_est = "{0}/last_alignments/{{sample_id}}.frag_len_est".format(config["out_dir"]),
        reference_flag= "{0}/last_index/index.done".format(config["out_dir"])
    params:
        last_index_basename="{0}/last_index/index".format(config["out_dir"]),
        mean = getMean,
        std = getSTD,
        scoring = getScoring
    threads: workflow.cores/len(config["reads"])
    output:
        "{0}/last_alignments/{{sample_id}}.tab".format(config["out_dir"])
    conda:
        "env/lastal.yaml"
    shell:
        """{interleave} {input.reads1} {input.reads2} | parallel --gnu --pipe -L4 -j {threads} "lastal -i1 -p {params.scoring} -F20 {params.last_index_basename} | last-pair-probs -f {params.mean} -s {params.std} -m 0.95 -d 0.1 | maf-convert tab" > {output} """

rule align_last_SE:
    input:
        reads1 = lambda wildcards: config["reads"][wildcards.sample_id][0],
        reference_flag= "{0}/last_index/index.done".format(config["out_dir"])
    params:
        last_index_basename="{0}/last_index/index".format(config["out_dir"]),
        scoring = getScoring
    threads: workflow.cores / len(config["reads"])
    output:
        "{0}/last_alignments/{{sample_id}}.tab".format(config["out_dir"])
    conda:
        "env/lastal.yaml"
    shell:
        """{convert_fastq_fasta_SE} {input.reads1} | parallel --gnu --pipe -L2 -j {threads} "lastal -p {params.scoring} -F20 {params.last_index_basename} | last-map-probs -m 0.95 | maf-convert tab" > {output} """
        #"{convert_fastq_fasta_SE} {input.reads1} | lastal -p {params.scoring} -F20 {params.last_index_basename} | last-map-probs -m 0.95 | maf-convert tab > {output} "


rule count_last: #Rule for getting count data from LAST alignments
    input:
        alignments = "{0}/last_alignments/{{sample_id}}.tab".format(config["out_dir"]),
        reference = config["reference_p"]
    params:
        mean = getMean,
        std = getSTD,
        singleEnd = lambda wildcards: len (config["reads"][wildcards.sample_id]) == 1
    output:
        "{0}/counts/{{sample_id}}/{{sample_id}}.counts".format(config["out_dir"])
    conda:
        "env/counting.yaml"
    shell:
        "python {seq_count} {input.reference} {input.alignments} {output} {params.singleEnd} --frag_len_mean {params.mean} --frag_len_std {params.std} "
